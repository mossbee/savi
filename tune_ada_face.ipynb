{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AdaFace Fine-tuning Guide\n",
    "\n",
    "This guide will walk you through the process of fine-tuning a pretrained AdaFace model, organizing your data, and creating an inference pipeline.\n",
    "\n",
    "## 1. Data Organization for Fine-tuning\n",
    "\n",
    "For fine-tuning, you'll need to organize your face images in a directory structure where each subfolder represents a different person/identity:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": ""
    }
   },
   "outputs": [],
   "source": [
    "dataset_root/\n",
    "├── person_1/\n",
    "│   ├── image1.jpg\n",
    "│   ├── image2.jpg\n",
    "│   └── ...\n",
    "├── person_2/\n",
    "│   ├── image1.jpg\n",
    "│   ├── image2.jpg\n",
    "│   └── ...\n",
    "└── ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Data Preparation Steps\n",
    "\n",
    "1. **Collect aligned face images**: Ensure all face images are aligned and cropped to 112x112 pixels\n",
    "2. **Organize by identity**: Create a folder for each person with their face images\n",
    "3. **Validation data**: Create a separate folder structure for validation if needed\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "bash"
    }
   },
   "outputs": [],
   "source": [
    "# Example directory structure\n",
    "my_face_dataset/\n",
    "├── imgs/                  # Training set\n",
    "│   ├── person_1/\n",
    "│   ├── person_2/\n",
    "│   └── ...\n",
    "└── val_images/            # Optional validation images\n",
    "    ├── agedb_30.bin       # Optional validation binaries\n",
    "    ├── lfw.bin\n",
    "    └── ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## 2. Fine-tuning Script\n",
    "\n",
    "Create a script `finetune.py` with the following content:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import argparse\n",
    "from datetime import datetime\n",
    "import sys\n",
    "sys.path.insert(0, os.path.dirname(os.getcwd()))\n",
    "\n",
    "def get_args():\n",
    "    parser = argparse.ArgumentParser(description='AdaFace fine-tuning')\n",
    "    \n",
    "    # Data parameters\n",
    "    parser.add_argument('--data_root', type=str, default='./data', help='Path to the data root directory')\n",
    "    parser.add_argument('--train_data_path', type=str, default='my_face_dataset/imgs', help='Path to training data relative to data_root')\n",
    "    parser.add_argument('--val_data_path', type=str, default='my_face_dataset', help='Path to validation data relative to data_root')\n",
    "    \n",
    "    # Model parameters\n",
    "    parser.add_argument('--arch', type=str, default='ir_50', choices=['ir_18', 'ir_34', 'ir_50', 'ir_101', 'ir_200'])\n",
    "    parser.add_argument('--pretrained_model', type=str, required=True, help='Path to the pretrained model')\n",
    "    \n",
    "    # Training parameters\n",
    "    parser.add_argument('--epochs', type=int, default=10, help='Number of fine-tuning epochs')\n",
    "    parser.add_argument('--batch_size', type=int, default=64)\n",
    "    parser.add_argument('--output_dir', type=str, default='./finetuned_models')\n",
    "    parser.add_argument('--lr', type=float, default=0.001, help='Lower learning rate for fine-tuning')\n",
    "    parser.add_argument('--gpus', type=int, default=1, help='Number of GPUs to use')\n",
    "    parser.add_argument('--use_16bit', action='store_true', help='Use 16-bit precision')\n",
    "    \n",
    "    # AdaFace specific parameters\n",
    "    parser.add_argument('--head', type=str, default='adaface')\n",
    "    parser.add_argument('--m', type=float, default=0.4, help='AdaFace margin parameter')\n",
    "    parser.add_argument('--h', type=float, default=0.333, help='AdaFace h parameter')\n",
    "    parser.add_argument('--s', type=float, default=64.0, help='AdaFace scale parameter')\n",
    "    parser.add_argument('--t_alpha', type=float, default=0.01)\n",
    "    \n",
    "    # Augmentation\n",
    "    parser.add_argument('--low_res_augmentation_prob', type=float, default=0.2)\n",
    "    parser.add_argument('--crop_augmentation_prob', type=float, default=0.2)\n",
    "    parser.add_argument('--photometric_augmentation_prob', type=float, default=0.2)\n",
    "    \n",
    "    # Other parameters\n",
    "    parser.add_argument('--distributed_backend', type=str, default='dp')\n",
    "    parser.add_argument('--num_workers', type=int, default=4)\n",
    "    parser.add_argument('--seed', type=int, default=42)\n",
    "    parser.add_argument('--save_all_models', action='store_true', help='Save all checkpoints')\n",
    "    parser.add_argument('--momentum', type=float, default=0.9)\n",
    "    parser.add_argument('--lr_milestones', type=str, default='5,8', help='Learning rate milestones')\n",
    "    parser.add_argument('--lr_gamma', type=float, default=0.1, help='Learning rate decay rate')\n",
    "    parser.add_argument('--accumulate_grad_batches', type=int, default=1)\n",
    "    parser.add_argument('--fast_dev_run', action='store_true')\n",
    "    parser.add_argument('--test_run', action='store_true')\n",
    "    parser.add_argument('--evaluate', action='store_false')\n",
    "    parser.add_argument('--use_wandb', action='store_true')\n",
    "    parser.add_argument('--use_mxrecord', action='store_false')\n",
    "    \n",
    "    return parser.parse_args()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    args = get_args()\n",
    "    \n",
    "    # Create a timestamped output directory\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    args.output_dir = os.path.join(args.output_dir, f\"{args.arch}_{timestamp}\")\n",
    "    os.makedirs(args.output_dir, exist_ok=True)\n",
    "    \n",
    "    # Set pretrained model path for loading\n",
    "    args.start_from_model_statedict = args.pretrained_model\n",
    "    \n",
    "    # Convert lr_milestones from string to list of ints\n",
    "    args.lr_milestones = [int(x) for x in args.lr_milestones.split(',')]\n",
    "    \n",
    "    # Import main after argument parsing to avoid circular imports\n",
    "    from main import main\n",
    "    main(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## 3. Run Fine-tuning\n",
    "\n",
    "To fine-tune a pretrained model:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "bash"
    }
   },
   "outputs": [],
   "source": [
    "python finetune.py \\\n",
    "    --data_root /path/to/your/data \\\n",
    "    --train_data_path my_face_dataset/imgs \\\n",
    "    --val_data_path my_face_dataset \\\n",
    "    --pretrained_model ./pretrained/adaface_ir50_ms1mv2.ckpt \\\n",
    "    --arch ir_50 \\\n",
    "    --batch_size 64 \\\n",
    "    --epochs 10 \\\n",
    "    --lr 0.001 \\\n",
    "    --gpus 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## 4. Inference Pipeline\n",
    "\n",
    "Create a file `embedding_pipeline.py` with the following code to load your fine-tuned model and extract face embeddings:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import net\n",
    "from face_alignment import align\n",
    "import argparse\n",
    "\n",
    "class AdaFaceInference:\n",
    "    def __init__(self, model_path, architecture='ir_50', device='cuda:0'):\n",
    "        \"\"\"\n",
    "        Initialize the AdaFace inference pipeline\n",
    "        \n",
    "        Args:\n",
    "            model_path: Path to the fine-tuned model checkpoint\n",
    "            architecture: Model architecture, one of ['ir_18', 'ir_34', 'ir_50', 'ir_101']\n",
    "            device: Device to run inference on ('cuda:0', 'cpu')\n",
    "        \"\"\"\n",
    "        self.device = torch.device(device)\n",
    "        \n",
    "        # Build model\n",
    "        self.model = net.build_model(architecture)\n",
    "        \n",
    "        # Load state dict\n",
    "        checkpoint = torch.load(model_path, map_location=self.device)\n",
    "        if 'state_dict' in checkpoint:\n",
    "            state_dict = checkpoint['state_dict']\n",
    "            # Remove 'model.' prefix if it exists\n",
    "            model_state_dict = {key.replace('model.', ''):val for key, val in state_dict.items() if key.startswith('model.')}\n",
    "            self.model.load_state_dict(model_state_dict)\n",
    "        else:\n",
    "            self.model.load_state_dict(checkpoint)\n",
    "            \n",
    "        self.model.to(self.device)\n",
    "        self.model.eval()\n",
    "        \n",
    "    def preprocess_image(self, image_path):\n",
    "        \"\"\"\n",
    "        Align face and preprocess the image\n",
    "        \n",
    "        Args:\n",
    "            image_path: Path to the input image\n",
    "            \n",
    "        Returns:\n",
    "            Preprocessed tensor ready for the model\n",
    "        \"\"\"\n",
    "        # Get aligned face\n",
    "        aligned_rgb_img = align.get_aligned_face(image_path)\n",
    "        if aligned_rgb_img is None:\n",
    "            raise ValueError(f\"Could not detect face in image: {image_path}\")\n",
    "            \n",
    "        # Convert to BGR and normalize\n",
    "        bgr_img = aligned_rgb_img[:, :, ::-1]  # RGB to BGR\n",
    "        bgr_tensor = torch.tensor([(bgr_img / 255. - 0.5) / 0.5]).float()\n",
    "        \n",
    "        return bgr_tensor, aligned_rgb_img\n",
    "    \n",
    "    def get_embedding(self, image_path, return_aligned_face=False):\n",
    "        \"\"\"\n",
    "        Extract embedding from a face image\n",
    "        \n",
    "        Args:\n",
    "            image_path: Path to the input image\n",
    "            return_aligned_face: Whether to return the aligned face image\n",
    "            \n",
    "        Returns:\n",
    "            Face embedding vector, and optionally the aligned face\n",
    "        \"\"\"\n",
    "        # Preprocess image\n",
    "        tensor, aligned_face = self.preprocess_image(image_path)\n",
    "        \n",
    "        # Get embedding\n",
    "        with torch.no_grad():\n",
    "            tensor = tensor.to(self.device)\n",
    "            embedding = self.model(tensor)\n",
    "            \n",
    "            # If model returns tuple (embedding, norm)\n",
    "            if isinstance(embedding, tuple):\n",
    "                embedding = embedding[0]\n",
    "                \n",
    "            # Normalize embedding\n",
    "            embedding = torch.nn.functional.normalize(embedding, p=2, dim=1)\n",
    "            embedding = embedding.cpu().numpy().flatten()\n",
    "        \n",
    "        if return_aligned_face:\n",
    "            return embedding, aligned_face\n",
    "        return embedding\n",
    "    \n",
    "    def compare_faces(self, image_path1, image_path2):\n",
    "        \"\"\"\n",
    "        Compare two faces and return similarity score\n",
    "        \n",
    "        Args:\n",
    "            image_path1: Path to the first image\n",
    "            image_path2: Path to the second image\n",
    "            \n",
    "        Returns:\n",
    "            Cosine similarity score between the two face embeddings\n",
    "        \"\"\"\n",
    "        emb1 = self.get_embedding(image_path1)\n",
    "        emb2 = self.get_embedding(image_path2)\n",
    "        \n",
    "        similarity = np.dot(emb1, emb2)\n",
    "        return similarity\n",
    "        \n",
    "    def visualize_comparison(self, image_path1, image_path2, title=None):\n",
    "        \"\"\"\n",
    "        Visualize two face images and their similarity\n",
    "        \n",
    "        Args:\n",
    "            image_path1: Path to the first image\n",
    "            image_path2: Path to the second image\n",
    "            title: Optional title for the plot\n",
    "            \n",
    "        Returns:\n",
    "            Matplotlib figure\n",
    "        \"\"\"\n",
    "        emb1, aligned1 = self.get_embedding(image_path1, return_aligned_face=True)\n",
    "        emb2, aligned2 = self.get_embedding(image_path2, return_aligned_face=True)\n",
    "        \n",
    "        similarity = np.dot(emb1, emb2)\n",
    "        \n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))\n",
    "        ax1.imshow(aligned1)\n",
    "        ax1.set_title(\"Image 1\")\n",
    "        ax1.axis('off')\n",
    "        \n",
    "        ax2.imshow(aligned2)\n",
    "        ax2.set_title(\"Image 2\")\n",
    "        ax2.axis('off')\n",
    "        \n",
    "        plt_title = f\"Similarity: {similarity:.4f}\"\n",
    "        if title:\n",
    "            plt_title = f\"{title}\\n{plt_title}\"\n",
    "        fig.suptitle(plt_title)\n",
    "        \n",
    "        return fig, similarity\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser(description='AdaFace inference')\n",
    "    parser.add_argument('--model_path', type=str, required=True, help='Path to fine-tuned model checkpoint')\n",
    "    parser.add_argument('--arch', type=str, default='ir_50', choices=['ir_18', 'ir_34', 'ir_50', 'ir_101'])\n",
    "    parser.add_argument('--image', type=str, required=True, help='Path to input image')\n",
    "    parser.add_argument('--compare_with', type=str, help='Path to image to compare with (optional)')\n",
    "    parser.add_argument('--device', type=str, default='cuda:0', help='Device to run inference on')\n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    # Initialize inference pipeline\n",
    "    inference = AdaFaceInference(model_path=args.model_path, architecture=args.arch, device=args.device)\n",
    "    \n",
    "    # Get embedding\n",
    "    if args.compare_with:\n",
    "        # Compare two images\n",
    "        fig, similarity = inference.visualize_comparison(args.image, args.compare_with)\n",
    "        plt.show()\n",
    "        print(f\"Similarity score: {similarity:.4f}\")\n",
    "    else:\n",
    "        # Get embedding for a single image\n",
    "        embedding, aligned_face = inference.get_embedding(args.image, return_aligned_face=True)\n",
    "        \n",
    "        # Display the aligned face and embedding\n",
    "        plt.imshow(aligned_face)\n",
    "        plt.title(\"Aligned Face\")\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "        \n",
    "        print(\"Face embedding (first 10 values):\", embedding[:10])\n",
    "        print(\"Embedding shape:\", embedding.shape)\n",
    "        print(\"Embedding L2 norm:\", np.linalg.norm(embedding))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## 5. Using the Inference Pipeline\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "bash"
    }
   },
   "outputs": [],
   "source": [
    "# Get embedding for a single image\n",
    "python embedding_pipeline.py --model_path ./finetuned_models/ir_50_20250331_121530/epoch=9-val_acc=0.98.ckpt --image path/to/face_image.jpg\n",
    "\n",
    "# Compare two face images\n",
    "python embedding_pipeline.py --model_path ./finetuned_models/ir_50_20250331_121530/epoch=9-val_acc=0.98.ckpt --image path/to/face1.jpg --compare_with path/to/face2.jpg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Additional Tips for Fine-tuning\n",
    "\n",
    "1. **Learning Rate**: Use a smaller learning rate than for training from scratch (0.001 or lower)\n",
    "2. **Epochs**: Fewer epochs are typically needed for fine-tuning (5-10)\n",
    "3. **Data Augmentation**: Keep augmentation probabilities lower for fine-tuning\n",
    "4. **Batch Size**: Adjust based on your GPU memory\n",
    "\n",
    "### Example Fine-tuning Script with Lower Learning Rate\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "bash"
    }
   },
   "outputs": [],
   "source": [
    "python finetune.py \\\n",
    "    --data_root /path/to/data \\\n",
    "    --train_data_path my_face_dataset/imgs \\\n",
    "    --val_data_path my_face_dataset \\\n",
    "    --pretrained_model ./pretrained/adaface_ir50_ms1mv2.ckpt \\\n",
    "    --arch ir_50 \\\n",
    "    --batch_size 64 \\\n",
    "    --lr 0.0005 \\\n",
    "    --lr_milestones 3,6,8 \\\n",
    "    --epochs 10 \\\n",
    "    --gpus 1 \\\n",
    "    --low_res_augmentation_prob 0.1 \\\n",
    "    --crop_augmentation_prob 0.1 \\\n",
    "    --photometric_augmentation_prob 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Make sure all your face images are aligned and cropped to 112x112 pixels. If not, you can use the MTCNN alignment process included in the AdaFace repo to preprocess your dataset before fine-tuning."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
