{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Host LLM on Kaggla/Colab for local inferencing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Starts a local server to serve a large language model (Qwen2.5-Coder-7B-Instruct), and uses ngrok to make the server accessible over the internet.\n",
    "\n",
    "*** sensitive information not included"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "bat"
    }
   },
   "outputs": [],
   "source": [
    "!pip install vllm transformers pyngrok\n",
    "!ngrok config add-authtoken ***\n",
    "!vllm serve Qwen/Qwen2.5-Coder-7B-Instruct --port 11434 --dtype=half --max-seq-len 4096 --tensor-parallel-size 2 & ngrok http 11434 --host-header=\"localhost:11434\" --log stdout --url ***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Base-line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Directly call the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use LangChain's [Structured outputs](https://python.langchain.com/docs/concepts/structured_outputs/) to make sure results in a structured format for further use. Batch queries for faster inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from pydantic import BaseModel, Field\n",
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "df = pd.read_csv('b6_test_data.csv')\n",
    "\n",
    "df_yet = pd.read_csv('submission.csv')\n",
    "# Save all values in the task_id column to a list\n",
    "task_id_list = df_yet['task_id'].tolist()\n",
    "\n",
    "\n",
    "def format_choices_with_letters(choices_str):\n",
    "    choices = eval(choices_str)\n",
    "    letters = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J']\n",
    "    return \"\\n\".join([f\"{letters[i]}. {choice}\" for i, choice in enumerate(choices)])\n",
    "\n",
    "\n",
    "# Set your vLLM endpoint as the OpenAI-compatible API\n",
    "os.environ[\"OPENAI_API_BASE\"] = \"https://glad-bass-wrongly.ngrok-free.app/v1\"\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"sk-fake-key\"  # Not used but required by LangChain\n",
    "\n",
    "# Define structured output using Pydantic\n",
    "class ExtractedInfo(BaseModel):\n",
    "    answer: str = Field(description=\"The output of the given code as one of A, B, C, D, or E\")\n",
    "    # reason: str = Field(description=\"A short reason explaining why the output is correct\")\n",
    "\n",
    "# Initialize the vLLM-backed ChatOpenAI model\n",
    "llm = ChatOpenAI(model_name=\"Qwen/Qwen2.5-Coder-7B-Instruct\",\n",
    "                 temperature=0,\n",
    "                 openai_api_base=os.environ[\"OPENAI_API_BASE\"],\n",
    "                 )\n",
    "\n",
    "llm = llm.with_structured_output(ExtractedInfo, method=\"json_mode\")\n",
    "\n",
    "# Function to analyze code in batches and return structured output\n",
    "def analyze_code_batch(batch):\n",
    "    prompts = []\n",
    "    for question, formatted_choices in batch:\n",
    "        prompt = [(\"system\", \"You are a helpful assistant that excels in answering multiple-choice programming questions. Let's explain step by step.\"),\n",
    "                  (\"human\",\n",
    "                   \"Analyze the following question and choices.\"\n",
    "                   \"Choose the correct answer from the following choices and return JSON with 'answer'.\\n\\n\"\n",
    "                   f\"Question:\\n{question}\\n\\n\"\n",
    "                   \"Which choice is correct?\\n\\n\"\n",
    "                   f\"Choices:\\n{formatted_choices}\\n\\n\")]\n",
    "        prompts.append(prompt)\n",
    "    return llm.batch_invoke(prompts)\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    batch_size = 10  # Define the batch size\n",
    "    results = []\n",
    "\n",
    "    # Filter rows that need processing\n",
    "    rows_to_process = df[~df['task_id'].isin(task_id_list)]\n",
    "\n",
    "    # Process in batches\n",
    "    for i in tqdm(range(0, len(rows_to_process), batch_size)):\n",
    "        batch = rows_to_process.iloc[i:i + batch_size]\n",
    "        batch_prompts = []\n",
    "\n",
    "        for _, row in batch.iterrows():\n",
    "            task_id = row['task_id']\n",
    "            question = row['question']\n",
    "            choices_str = row['choices']\n",
    "            formatted_choices = format_choices_with_letters(choices_str)\n",
    "            batch_prompts.append((question, formatted_choices))\n",
    "\n",
    "        try:\n",
    "            batch_results = analyze_code_batch(batch_prompts)\n",
    "            for row, result in zip(batch.iterrows(), batch_results):\n",
    "                task_id = row[1]['task_id']\n",
    "                results.append({\n",
    "                    'task_id': task_id,\n",
    "                    'answer': result.answer,\n",
    "                })\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing batch starting at index {i}: {e}\")\n",
    "            continue\n",
    "\n",
    "    # Save the results to a CSV file\n",
    "    output_df = pd.DataFrame(results)\n",
    "    output_df.to_csv('submission.csv', index=False, mode='a', header=not os.path.exists('submission.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Few-shot prompting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find similar questions in the training data for a given test question. Include them all when inference. Functions to do so."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This can furthur improved by generate the reason for the answer, but I didn't have much time to do that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "def load_data(train_path: str, test_path: str):\n",
    "    train_df = pd.read_csv(train_path)\n",
    "    test_df = pd.read_csv(test_path)\n",
    "    \n",
    "    train_df['full_text'] = train_df['question'] + ' ' + train_df['choices'].fillna('')\n",
    "    return train_df, test_df\n",
    "\n",
    "def compute_embeddings(train_df, test_df, model_name='intfloat/multilingual-e5-large-instruct'):\n",
    "    model = SentenceTransformer(model_name)\n",
    "    \n",
    "    train_texts = train_df['full_text'].tolist()\n",
    "    test_texts = test_df['question'].tolist()\n",
    "    \n",
    "    train_embeddings = model.encode(train_texts, convert_to_tensor=True, normalize_embeddings=True)\n",
    "    test_embeddings = model.encode(test_texts, convert_to_tensor=True, normalize_embeddings=True)\n",
    "    \n",
    "    return train_embeddings, test_embeddings\n",
    "\n",
    "def precompute_similarity(train_embeddings, test_embeddings):\n",
    "    similarity_matrix = (test_embeddings @ train_embeddings.T) * 100\n",
    "    return similarity_matrix.cpu().numpy() if torch.is_tensor(similarity_matrix) else similarity_matrix\n",
    "\n",
    "def retrieve_top_k(similarity_matrix, train_df, k=3):\n",
    "    top_k_indices = np.argsort(-similarity_matrix, axis=1)[:, :k]\n",
    "    top_k_task_ids = train_df['task_id'].values[top_k_indices]\n",
    "    return top_k_task_ids\n",
    "\n",
    "def find_top_k_for_test_id(test_task_id, test_df, top_k_task_ids):\n",
    "    test_index = test_df.index[test_df['task_id'] == test_task_id].tolist()\n",
    "    if not test_index:\n",
    "        return []\n",
    "    return top_k_task_ids[test_index[0]].tolist()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Few-show prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "\n",
    "class ExtractedInfo(BaseModel):\n",
    "    answer: str = Field(description=\"The output of the given code as one of A, B, C, D, or E\")\n",
    "\n",
    "def generate_answers(test_df, train_df, top_k_task_ids, model_name=\"Qwen/Qwen2.5-Coder-7B-Instruct\", batch_size=10):\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=\"auto\", device_map=\"auto\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    \n",
    "    system_prompt = \"\"\"You are a helpful AI assistant tasked with answering multiple-choice questions about coding.\n",
    "\n",
    "                        I will provide you with questions and their possible answer choices. I will also give you several examples. For each question:\n",
    "                        1. Read the question carefully.\n",
    "                        2. Examine all answer options.\n",
    "                        3. Select the most accurate answer.\n",
    "                        4. Respond ONLY with the **letter** of the correct choice (e.g., 'A', 'B', 'C', etc.).\n",
    "                        5. Do not include any explanations, text, or anything other than the letter of the correct answer.\n",
    "\n",
    "                        Here are some examples:\n",
    "\n",
    "                        1. Question: {question1}\n",
    "                        Choices:\n",
    "                        {choice1}\n",
    "                        Answer: {answer1}\n",
    "\n",
    "                        2. Question: {question2}\n",
    "                        Choices:\n",
    "                        {choice2}\n",
    "                        Answer: {answer2}\n",
    "\n",
    "                        3. Question: {question3}\n",
    "                        Choices:\n",
    "                        {choice3}\n",
    "                        Answer: {answer3}\n",
    "\n",
    "                        **Your answer must be only a single letter corresponding to the correct choice, with no additional content.**\n",
    "                        Now, please answer the following question:\n",
    "                        \"\"\"\n",
    "    \n",
    "    answers = []\n",
    "    for i in range(0, len(test_df), batch_size):\n",
    "        batch = test_df.iloc[i:i + batch_size]\n",
    "        prompts = []\n",
    "        \n",
    "        for _, row in batch.iterrows():\n",
    "            top_k_examples = top_k_task_ids[_]\n",
    "            examples = \"\\n\\n\".join([\n",
    "                f\"Question: {train_df.loc[train_df['task_id'] == tid, 'question'].values[0]}\\nChoices:\\n{train_df.loc[train_df['task_id'] == tid, 'choices'].values[0]}\\nAnswer: {train_df.loc[train_df['task_id'] == tid, 'answer'].values[0]}\"\n",
    "                for tid in top_k_examples\n",
    "            ])\n",
    "            \n",
    "            prompt = f\"{system_prompt}\\n\\n{examples}\\n\\nHere is the question:\\nQuestion:\\n{row['question']}\\nChoices:\\n{row['choices']}\"\n",
    "            messages = [{\"role\": \"system\", \"content\": \"You are Qwen, created by Alibaba Cloud. You are a helpful assistant.\"},\n",
    "                        {\"role\": \"user\", \"content\": prompt}]\n",
    "            \n",
    "            text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "            prompts.append(text)\n",
    "        \n",
    "        # Tokenize the batch\n",
    "        model_inputs = tokenizer(prompts, return_tensors=\"pt\", padding=True, truncation=True).to(model.device)\n",
    "        \n",
    "        # Generate responses for the batch\n",
    "        generated_ids = model.generate(**model_inputs, max_new_tokens=10)\n",
    "        responses = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "        \n",
    "        # Parse responses and append to answers\n",
    "        for row, response in zip(batch.iterrows(), responses):\n",
    "            task_id = row[1]['task_id']\n",
    "            structured_output = ExtractedInfo.parse_raw(response.strip())\n",
    "            answers.append((task_id, structured_output.answer))\n",
    "    \n",
    "    # Save the results to a CSV file\n",
    "    submission_df = pd.DataFrame(answers, columns=['task_id', 'answer'])\n",
    "    submission_df.to_csv('submission.csv', index=False)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train_df, test_df = load_data('b6_train_data.csv', 'b6_test_data.csv')\n",
    "    train_embeddings, test_embeddings = compute_embeddings(train_df, test_df)\n",
    "    similarity_matrix = precompute_similarity(train_embeddings, test_embeddings)\n",
    "    top_k_task_ids = retrieve_top_k(similarity_matrix, train_df)\n",
    "    generate_answers(test_df, train_df, top_k_task_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code running for executable questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use regex to find out executable question. And extract the code snippet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import re\n",
    "\n",
    "def extract_questions(input_csv, output_csv):\n",
    "    patterns = [\n",
    "        r\"^Question: What will be output\\??\",\n",
    "        r\"^Question: What will be the output\\??\",\n",
    "        r\"^Question: What is the output\\??\",\n",
    "        r\"^Question: What would be the output\\??\"\n",
    "    ]\n",
    "\n",
    "    with open(input_csv, newline='', encoding='utf-8') as infile, \\\n",
    "         open(output_csv, mode='w', newline='', encoding='utf-8') as outfile:\n",
    "\n",
    "        reader = csv.DictReader(infile)\n",
    "        fieldnames = ['task_id', 'extracted_text']\n",
    "        writer = csv.DictWriter(outfile, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "\n",
    "        for row in reader:\n",
    "            question = row['question'].strip()\n",
    "            task_id = row['task_id']\n",
    "\n",
    "            for pattern in patterns:\n",
    "                match = re.match(pattern, question)\n",
    "                if match:\n",
    "                    extracted_text = question[question.find('?')+1:].strip()\n",
    "                    writer.writerow({'task_id': task_id, 'extracted_text': extracted_text})\n",
    "                    break\n",
    "\n",
    "extract_questions('b6_test_data.csv', 'b6_code_extracted.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
